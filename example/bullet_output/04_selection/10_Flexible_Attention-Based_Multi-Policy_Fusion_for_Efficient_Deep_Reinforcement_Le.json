{
  "paper_id": "10_Flexible_Attention-Based_Multi-Policy_Fusion_for_Efficient_Deep_Reinforcement_Le",
  "selected_nodes": [
    {
      "unique_id": "d50b5c69-dacb-4ff3-bfb1-14f9d2dc3716",
      "score": 3.5747175627450734,
      "pagerank": 3.5747175627450734,
      "section_title": "1 Introduction",
      "context": "Our contributions are: - We introduce KGRL, an RL paradigm studying how agents learn with external policies while being knowledge-acquirable, sample-efficient, generalizable, compositional, and incremental. - We propose KIAN, an actor model for KGRL that fuses multiple knowledge policies with better flexibility and addresses entropy imbalance for more efficient exploration. - We demonstrate in experiments that KIAN outperforms other methods incorporating external knowledge policies under different environmental setups."
    },
    {
      "unique_id": "a5dbf67f-7ddf-42e4-b5e4-074d087c4af1",
      "score": 2.511040042092011,
      "pagerank": 2.511040042092011,
      "section_title": "6 Conclusion and Discussion",
      "context": "Our research represents an initial step towards the overarching goal of KGRL: learning a knowledge set with a diverse range of policies. These knowledge policies can be shared across various environments and continuously expanded, allowing artificial agents to flexibly query and learn from them. We provide detailed discussions on the broader impact of this work and outline potential directions of future research in Appendix D."
    },
    {
      "unique_id": "63f05123-2d8f-416e-9125-358e417c8bb4",
      "score": 2.0284647812787364,
      "pagerank": 2.0284647812787364,
      "section_title": "4 Knowledge-Inclusive Attention Network",
      "context": "We propose Knowledge-Inclusive Attention Network (KIAN) as an actor for KGRL. KIAN can be end-to-end trained with various RL algorithms. Illustrated in Figure 2, KIAN comprises three components: an inner actor, knowledge keys, and a query. In this section, we first describe the architecture of KIAN and its action-prediction operation. Then we introduce entropy imbalance, a problem that emerges in maximum entropy KGRL, and propose modified policy distributions for KIAN to alleviate this issue."
    },
    {
      "unique_id": "d89c6bfe-2bde-4375-8db9-68b9be1e2e1a",
      "score": 1.8497531493695085,
      "pagerank": 1.8497531493695085,
      "section_title": "6 Conclusion and Discussion",
      "context": "This work introduces KGRL, an RL paradigm aiming to enhance efficient and flexible learning by harnessing external policies. We propose KIAN as an actor model for KGRL, which predicts an action by fusing multiple policies with an embedding-based attention operation. Furthermore, we propose modifications to KIAN's policy distributions to address entropy imbalance, which hinders efficient exploration with external policies in maximum entropy KGRL. Our experimental findings demonstrate that KIAN outperforms alternative methods incorporating external policies regarding sample efficiency, generalizability, and compositional and incremental learning.\nHowever, it is essential to acknowledge a limitation not addressed in this work. The efficiency of KIAN, as well as other existing KGRL methods, may decrease when dealing with a large external knowledge set containing irrelevant policies. This issue is examined and discussed in Appendix F. Efficiently handling extensive sets of external policies is left for future research."
    },
    {
      "unique_id": "743a0b16-2df1-4282-80ab-64d99e1d0b03",
      "score": 0.7029796710873434,
      "pagerank": 0.7029796710873434,
      "section_title": "1 Introduction",
      "context": "We propose a simple yet effective actor model, Knowledge-Inclusive Attention Network (KIAN), for KGRL. KIAN consists of three components: (1) an internal policy that learns a self-developed strategy, (2) embeddings that represent each policy, and (3) a query that performs embedding-based attentive action prediction to fuse the internal and external policies. The policy-embedding and query design in KIAN is crucial, as it enables the model to be incremental by unifying policy representations and separating them from the policy-fusing process. Consequently, updating or adding policies to KIAN has minimal effect on its architecture and does not require retraining the entire network. Additionally, KIAN addresses the problem of entropy imbalance in KGRL, where agents tend to choose only a few sub-optimal policies from the knowledge set. We provide mathematical evidence that entropy imbalance can prevent agents from exploring the environment with multiple policies. Then we introduce a new approach for modeling external-policy distributions to mitigate this issue.\nThrough experiments on grid navigation [5] and robotic manipulation [24] tasks, KIAN outperforms alternative methods incorporating external policies in terms of sample efficiency as well as the ability to do compositional and incremental learning. Furthermore, our analyses suggest that KIAN has better generalizability when applied to environments that are either simpler or more complex."
    },
    {
      "unique_id": "efffd628-49b3-4a43-808f-2af2e10ce478",
      "score": 0.4416130677409977,
      "pagerank": 0.4416130677409977,
      "section_title": "5 Experiments",
      "context": "In the final experiments, we test different methods in the compositional and incremental learning setting. We modify RL, KoGuN, and A2T to fit into this setting; details can be found in Appendix C. The experiments follow the inter-task setup: (1) We randomly select a pair of tasks  $(\\mathcal{M}_k^1,\\mathcal{M}_k^2)$ . (2) An agent learns a policy to solve  $\\mathcal{M}_k^1$  with  $\\mathcal{G}^{init}$  fixed, as done in Section 5.1. (3) The learned (internal) policy,  $\\pi_{in}^{1}$ , is added to the external knowledge set,  $\\mathcal{G} = \\mathcal{G}^{init} \\cup \\{\\pi_{in}^{1}\\}$ . (4) The same agent learns a policy to solve  $\\mathcal{M}_k^2$  with  $\\mathcal{G}$ . Each experiment is run with ten random seeds."
    },
    {
      "unique_id": "713104fc-c8a7-4fa3-8f1a-8e93692c3026",
      "score": 0.41161162072874097,
      "pagerank": 0.41161162072874097,
      "section_title": "5 Experiments",
      "context": "In our ablation study, we investigate (1) the impact of entropy imbalance on the performance of maximum entropy KGRL and (2) whether the proposed modifications to external policy distributions in Section 4.4 can alleviate the issue. Figure 5 shows the learning curves comparing KIAN's performance with and without addressing the entropy-imbalance issue. The results demonstrate that when not addressing the issue using equation (5) or (8), KIAN fails to fully capitalize on the guidance offered by external policies. We also draw two noteworthy conclusions from the figure: (1) For discrete decision-making tasks, the detrimental impact of entropy imbalance becomes more evident as task complexity increases. (2) For continuous-control tasks, entropy imbalance can degrade KIAN's performance and make it perform worse than pure RL without external policies, as shown by the results of FetchPickAndPlace and FetchPush. This phenomenon can be attributed to Proposition 4.3. In contrast, by adjusting KIAN's external policy distributions using equation (5) or (8), a KGRL agent can efficiently harness external policies to solve a given task."
    },
    {
      "unique_id": "050e5200-9524-468d-81cc-3fadc53c5e64",
      "score": 0.3542600149939414,
      "pagerank": 0.3542600149939414,
      "section_title": "5 Experiments",
      "context": "We evaluate KIAN on two sets of environments with discrete and continuous action spaces: MiniGrid [5] and OpenAI-Robotics [24]. Through experiments, we answer the following four questions: [Sample Efficiency] Does KIAN require fewer training samples to solve a task than other external-policy-inclusive methods? [Generalizability] Can KIAN trained on one task be directly used to solve another task? [Compositional and Incremental Learning] Can KIAN combine previously learned knowledge keys and inner policies to learn a new task? After adding more external policies to  $\\mathcal{G}$ , can most of the components from a trained KIAN be reused for learning?\nFor comparison, we implement the following five methods as our baselines: behavior cloning (BC) [3], RL [10, 29], RL+BC [21], KoGuN [36], and A2T [27]. KoGuN and A2T are modified to be compositional and applicable in both discrete and continuous action spaces. Moreover, all methods (BC, RL+BC, KoGuN, A2T, and KIAN) are equipped with the same initial external knowledge set,  $\\mathcal{G}^{init}$ , for each task. This knowledge set comprises sub-optimal if-else-based programs that cannot complete a task themselves, e.g., pickup_a_key or move_forward_to_the_goal.  $\\mathcal{G}^{init}$  will be expanded with learned policies in compositional- and incremental-learning experiments. We provide the experimental details in Appendix B.\nWe study the sample efficiency of baselines and KIAN under the intra-task setup, where an agent learns a single task with the external knowledge set  $\\mathcal{G}^{init}$  fixed. Figure 3 plots the learning curves in different environments. All experiments in these figures are run with ten random seeds, and each error band is a  $95\\%$  confidence interval. The results of BC show that the external knowledge policies are sub-optimal for all environments. Given sub-optimal external knowledge, only KIAN shows success in all environments. In general, improvement of KIAN over baselines is more apparent when the task is more complex, e.g., Empty  $<$  Unlock  $<$  DoorKey and Push  $<$  Pick-and-Place. Moreover, KIAN is more stable than baselines in most environments. Note that in continuous-control tasks (Push, Slide, and Pick-and-Place), A2T barely succeeds since it does not consider the entropy imbalance issue introduced in Proposition 4.2 and 4.3. These results suggest that KIAN can more efficiently explore the environment with external knowledge policies and fuse multiple policies to solve a task.\nNext, we evaluate the generalizability of all methods under simple-to-complex (S2C) and complex-to-simple (C2S) setups, where the former trains a policy in a simple task and test it in a complex one, and the latter goes the opposite way. All generalizability experiments are run with the same policies as in Section 5.1. Table 1 and 2 show that KIAN outperforms other baselines in most experiments, and its results have a smaller variance (see Table 3 to 5 in Appendix E). These results demonstrate that KIAN's flexibility in incorporating external policies improves generalizability."
    },
    {
      "unique_id": "0e6950ba-4799-4d67-9a8f-60345a773102",
      "score": 0.3542600149939414,
      "pagerank": 0.3542600149939414,
      "section_title": "2 Related Work",
      "context": "Another research direction in RL focuses on utilizing sub-optimal external policies instead of demonstrations to improve sample efficiency [25, 27, 36]. For instance, Zhang et al. [36] proposed Knowledge-Guided Policy Network (KoGuN) that learns a neural network policy from fuzzy-rule controllers. The neural network concatenates a state and all actions suggested by fuzzy-rule controllers as an input and outputs a refined action. While effective, this method puts restrictions on the representation of a policy to be a fuzzy logic network. On the other hand, Rajendran et al. [27] presented A2T (Attend, Adapt, and Transfer), an attentive deep architecture that fuses multiple policies and does not restrict the form of a policy. These policies can be non-primitive, and a learnable internal policy is included. In A2T, an attention network takes a state as an input and outputs the weights of all policies. The agent then samples an action from the fused distribution based on these weights. The methods KoGuN and A2T are most related to our work. Based on their success, KIAN further relaxes their requirement of retraining for incremental learning since both of them depend on the preset number of policies. Additionally, our approach mitigates the entropy imbalance issue, which can lead to inefficient exploration and was not addressed by KoGuN and A2T.\nThere exist other RL frameworks, such as hierarchical RL (HRL), that tackle tasks involving multiple policies. However, these frameworks are less closely related to our work compared to the previously mentioned methods. HRL approaches aim to decompose a complex task into a hierarchy of sub-tasks and learn a sub-policy for each sub-task [2, 6, 13, 16-18, 20, 25, 31, 33]. On the other hand, KGRL methods, including KoGuN, A2T, and KIAN, aim to address a task by observing a given set of external policies. These policies may offer partial solutions, be overly intricate, or even have limited relevance to the task at hand. Furthermore, HRL methods typically apply only one sub-policy to the environment at each time step based on the high-level policy, which determines the sub-task the agent is currently addressing. In contrast, KGRL seeks to simultaneously apply multiple policies within a single time step by fusing them together."
    },
    {
      "unique_id": "8dc064f4-0053-4eb9-926c-1e7774e0e193",
      "score": 0.3542600149939414,
      "pagerank": 0.3542600149939414,
      "section_title": "Abstract",
      "context": "Reinforcement learning (RL) agents have long sought to approach the efficiency of human learning. Humans are great observers who can learn by aggregating external knowledge from various sources, including observations from others' policies of attempting a task. Prior studies in RL have incorporated external knowledge policies to help agents improve sample efficiency. However, it remains non-trivial to perform arbitrary combinations and replacements of those policies, an essential feature for generalization and transferability. In this work, we present Knowledge-Grounded RL (KGRL), an RL paradigm fusing multiple knowledge policies and aiming for human-like efficiency and flexibility. We propose a new actor architecture for KGRL, Knowledge-Inclusive Attention Network (KIAN), which allows free knowledge rearrangement due to embedding-based attentive action prediction. KIAN also addresses entropy imbalance, a problem arising in maximum entropy KGRL that hinders an agent from efficiently exploring the environment, through a new design of policy distributions. The experimental results demonstrate that KIAN outperforms alternative methods incorporating external knowledge policies and achieves efficient and flexible learning. Our implementation is available at https://github.com/Pascalson/KGRL.git."
    },
    {
      "unique_id": "51c17f95-d0f5-45cb-95d4-8d1982d21571",
      "score": 0.3542600149939414,
      "pagerank": 0.3542600149939414,
      "section_title": "4 Knowledge-Inclusive Attention Network",
      "context": "Proposition 4.1, 4.2, and 4.3 indicate that in maximum entropy KGRL, (1) the agent will pay more attention to the policy with large entropy, and (2) in continuous control, an agent with a learnable internal policy will rely on this policy and separate it as far away as possible from other policies. The consistently imbalanced attention prevents the agent from exploring the environment with other policies that might provide helpful suggestions to solve the task. Furthermore, in continuous control, the distribution separation can make  $\\pi$  perform even worse than learning without any external knowledge. The reason is that external policies, although possibly being sub-optimal for the task, might be more efficient in approaching the goal, and moving away from those policies means being less efficient when exploring the environment.\nProposition 4.1 and 4.2 show that fusing multiple policies with equation (3) can make a KGRL agent rely on a learnable internal policy for exploration. However, the uniformity of the internal policy is often desired since it encourages exploration in the state-action space that is not covered by external policies. Therefore, we keep the internal policy unchanged and propose methods to modify external policy distributions in KIAN to resolve the entropy imbalance issue. We provide the detailed learning algorithm of KGRL with KIAN in Appendix A.6.\nDiscrete Policy Distribution. We modify a fusion of discrete policy distributions in equation (3) as $$ \\pi (\\cdot | \\mathbf {s} _ {t}) = \\hat {w} _ {t, i n} \\pi_ {i n} (\\cdot | \\mathbf {s} _ {t}) + \\sum_ {j = 1} ^ {n} \\hat {w} _ {t, g _ {j}} \\operatorname {s o f t m a x} \\left(\\beta_ {t, g _ {j}} \\pi_ {g _ {j}} (\\cdot | \\mathbf {s} _ {t})\\right), \\tag {5} $$ $$ w _ {t, i n} = \\frac {\\Phi (\\mathbf {s} _ {t}) \\cdot \\mathbf {k} _ {i n}}{\\| \\Phi (\\mathbf {s} _ {t}) \\| _ {2} \\| \\mathbf {k} _ {i n} \\| _ {2}}, w _ {t, g _ {j}} = \\frac {\\Phi (\\mathbf {s} _ {t}) \\cdot \\mathbf {k} _ {g _ {j}}}{\\| \\Phi (\\mathbf {s} _ {t}) \\| _ {2} \\| \\mathbf {k} _ {g _ {j}} \\| _ {2}}, \\tag {6} $$ $$ \\beta_ {t, g _ {j}} = \\| \\Phi (\\mathbf {s} _ {t}) \\| _ {2} \\| \\mathbf {k} _ {g _ {j}} \\| _ {2} \\quad \\forall j \\in \\{1, \\dots , n \\}, \\tag {7} $$ where  $\\beta_{t,g_j} \\in \\mathbb{R}$  is a state-and-knowledge dependent variable that scales  $\\pi_{g_j}(\\cdot|\\mathbf{s}_t)$  to change its uniformity after passing through softmax. If the value of  $\\beta_{t,g_j}$  decreases, the uniformity, i.e., the entropy, of softmax  $(\\beta_{t,g_j}\\pi_{g_j}(\\cdot|\\mathbf{s}_t))$  increases. By introducing  $\\beta_{t,g_j}$ , the entropy of knowledge policies becomes adjustable, resulting in reduced bias towards the internal policy during exploration.\nContinuous Action Probability. We modify the probability of sampling  $\\mathbf{a}_t \\in \\mathbb{R}^{d_a}$  from a continuous  $\\pi(\\cdot|\\mathbf{s}_t)$  in equation (3) as $$ \\pi \\left(\\mathbf {a} _ {t} \\mid \\mathbf {s} _ {t}\\right) = \\hat {w} _ {i n} \\pi_ {i n} \\left(\\mathbf {a} _ {t, i n} \\mid \\mathbf {s} _ {t}\\right) + \\sum_ {j = 1} ^ {n} \\hat {w} _ {g _ {j}} \\pi_ {g _ {j}} \\left(\\boldsymbol {\\mu} _ {t, g _ {j}} \\mid \\mathbf {s} _ {t}\\right), \\tag {8} $$ where  $\\mathbf{a}_{t,in} \\sim \\pi_{in}(\\cdot|\\mathbf{s}_t)$  and  $\\pmb{\\mu}_{t,g_j} \\in \\mathbb{R}^{d_a}$  is the mean of  $\\pi_{g_j}(\\cdot|\\mathbf{s}_t)$ . We show in the next proposition that equation (8) is an approximation of $$ \\pi \\left(\\mathbf {a} _ {t} \\mid \\mathbf {s} _ {t}\\right) = \\hat {w} _ {i n} \\pi_ {i n} \\left(\\mathbf {a} _ {t} \\mid \\mathbf {s} _ {t}\\right) + \\Sigma_ {j = 1} ^ {n} \\hat {w} _ {g _ {j}} \\pi_ {g _ {j}} \\left(\\mathbf {a} _ {t} \\mid \\mathbf {s} _ {t}\\right), \\tag {9} $$ which is the exact probability of sampling  $\\mathbf{a}_t\\in \\mathbb{R}^{d_a}$  from a continuous  $\\pi (\\cdot |\\mathbf{s}_t)$  in equation (3)."
    },
    {
      "unique_id": "6f8007c6-5da3-4804-9ee2-f35e50dab668",
      "score": 0.3542600149939414,
      "pagerank": 0.3542600149939414,
      "section_title": "4 Knowledge-Inclusive Attention Network",
      "context": "A continuous policy fused as equation (3) becomes a mixture of normal distributions. To sample an action from this mixture of distributions without losing the important information provided by each distribution, we choose only one knowledge policy according to the weights and sample an action from it. We first sample an element from the set $\\{in, g_1, \\ldots, g_n\\}$  according to the weights,  $\\{\\hat{w}_{t,in}, \\hat{w}_{t,g_1}, \\ldots, \\hat{w}_{t,g_n}\\}$ , using Gumbel softmax [12]:  $e \\sim \\text{gumbel\\_softmax}([\\hat{w}_{t,in}, \\hat{w}_{t,g_1}, \\ldots, \\hat{w}_{t,g_n}]^\\top)$ , in order to make KIAN differentiable everywhere. Then given a state  $\\mathbf{s}_t \\in S$ , an action is sampled from the knowledge policy,  $\\mathbf{a}_t \\sim \\pi_e(\\cdot|\\mathbf{s}_t)$ , using the reparameterization trick. However, fusing multiple policies as equation (3) will make an agent biased toward a small set of knowledge policies when exploring the environment in the context of maximum entropy KGRL.\nMaximizing entropy is a commonly used approach to encourage exploration in RL [9, 10, 37]. However, in maximum entropy KGRL, when the entropy of policy distributions are different from one another, it leads to the problem of entropy imbalance. Entropy imbalance is a phenomenon in which an agent consistently selects only a single or a small set of knowledge policies. We show this in math by first revisiting the formulation of maximum entropy RL. In maximum entropy RL, an entropy term is added to the standard RL objective as  $\\pi^{*} = \\operatorname{argmax}_{\\pi} \\sum_{t} \\mathbb{E}_{(\\mathbf{s}_{t}, \\mathbf{a}_{t} \\sim \\pi)}[R(\\mathbf{s}_{t}, \\mathbf{a}_{t}) + \\alpha H(\\pi(\\cdot | \\mathbf{s}_{t}))]$  [9, 10], where  $\\alpha \\in \\mathbb{R}$  is a hyperparameter, and  $H(\\cdot)$  represents the entropy of a distribution. By maximizing  $\\alpha H(\\pi(\\cdot | \\mathbf{s}_{t}))$ , the policy becomes more uniform since the entropy of a probability distribution is maximized when it is a uniform distribution [19]. With this in mind, we show that in maximum entropy KGRL, some of the weights in  $\\{\\hat{w}_{t,in}, \\hat{w}_{t,g_1}, \\dots, \\hat{w}_{t,g_n}\\}$  might always be larger than others. We provide the proofs of all propositions in Appendix A.\nProposition 4.1 (Entropy imbalance in discrete decision-making). Assume that a  $d_{a}$ -dimensional probability simplex  $\\pi \\in \\Delta^{d_a}$  is fused by  $\\{\\pi_1,\\dots ,\\pi_m\\}$  and  $\\{\\hat{w}_1,\\ldots ,\\hat{w}_m\\}$  following equation (3), where  $\\pi_j\\in \\Delta^{d_a}$ ,  $\\hat{w}_j\\geq 0$ $\\forall j\\in \\{1,\\dots,m\\}$  and  $\\sum_{j = 1}^{m}\\hat{w}_{j} = 1$ . If the entropy of  $\\pi$  is maximized and  $\\| \\pi_1\\|_{\\infty}\\ll \\| \\pi_2\\|_{\\infty},\\| \\pi_1\\|_{\\infty}\\ll \\| \\pi_3\\|_{\\infty},\\dots,\\| \\pi_1\\|_{\\infty}\\ll \\| \\pi_m\\|_{\\infty}$ , then  $\\hat{w}_1\\rightarrow 1$ We show in Proposition A.1 that if  $\\pi_1$  is more uniform than  $\\pi_j$ , then  $\\| \\pi_1 \\|_{\\infty} < \\| \\pi_j \\|_{\\infty}$ . Proposition 4.2 (Entropy imbalance in continuous control). Assume a one-dimensional policy distribution  $\\pi$  is fused by $$ \\pi = \\hat {w} _ {1} \\pi_ {1} + \\hat {w} _ {2} \\pi_ {2}, \\text {w h e r e} \\pi_ {j} = \\mathcal {N} \\left(\\mu_ {j}, \\sigma_ {j} ^ {2}\\right), \\hat {w} _ {j} \\geq 0 \\forall j \\in \\{1, 2 \\}, \\text {a n d} \\hat {w} _ {1} + \\hat {w} _ {2} = 1. \\tag {4} $$ If the variance of  $\\pi$  is maximized, and  $\\sigma_1^2 \\gg \\sigma_2^2$  and  $\\sigma_1^2 \\gg (\\mu_1 - \\mu_2)^2$ , then  $\\hat{w}_1 \\to 1$ . We can also infer from Proposition 4.2 that the variance of  $\\pi$  defined in equation (4) depends on the distance between  $\\mu_{1}$  and  $\\mu_{2}$ , which leads to Proposition 4.3. Proposition 4.3 (Distribution separation in continuous control). Assume a one-dimensional policy distribution  $\\pi$  is fused by equation (4). If  $\\hat{w}_1, \\hat{w}_2, \\sigma_1^2$ , and  $\\sigma_2^2$  are fixed, then maximizing the variance of  $\\pi$  will increase the distance between  $\\mu_1$  and  $\\mu_2$ ."
    },
    {
      "unique_id": "ed3854e6-ebba-4021-ba1e-4873eefccbdc",
      "score": 0.3542600149939414,
      "pagerank": 0.3542600149939414,
      "section_title": "3 Problem Formulation",
      "context": "Our goal is to investigate how RL can be grounded on any given set of external knowledge policies to achieve knowledge-acquirable, sample-efficient, generalizable, compositional, and incremental properties. We refer to this RL paradigm as Knowledge-Grounded Reinforcement Learning (KGRL). A KGRL problem is a sequential decision-making problem that involves an environment, an agent, and a set of external policies. It can be mathematically formulated as a Knowledge-Grounded Markov Decision Process (KGMDP), which is defined by a tuple  $\\mathcal{M}_k = (\\mathcal{S},\\mathcal{A},\\mathcal{T},R,\\rho ,\\gamma ,\\mathcal{G})$ , where  $\\mathcal{S}$  is the state space,  $\\mathcal{A}$  is the action space,  $\\mathcal{T}:S\\times \\mathcal{A}\\times S\\to \\mathbb{R}$  is the transition probability distribution,  $R$  is the reward function,  $\\rho$  is the initial state distribution,  $\\gamma$  is the discount factor, and  $\\mathcal{G}$  is the set of external knowledge policies. An external knowledge set  $\\mathcal{G}$  contains  $n$  knowledge policies,  $\\mathcal{G} = \\{\\pi_{g_1},\\ldots ,\\pi_{g_n}\\}$ . Each knowledge policy is a function that maps from the state space to the action space,  $\\pi_{g_j}(\\cdot |\\cdot):S\\rightarrow A,\\forall j = 1,\\dots ,n$ . A knowledge mapping is not necessarily designed for the original Markov Decision Process (MDP), which is defined by the tuple  $\\mathcal{M} = (\\mathcal{S},\\mathcal{A},\\mathcal{T},\\mathcal{R},\\rho ,\\gamma)$ . Therefore, applying  $\\pi_{g_j}$  to  $\\mathcal{M}$  may result in a poor expected return.\nThe goal of KGRL is to find an optimal policy  $\\pi^{*}(\\cdot |\\cdot ;\\mathcal{G}):S\\to \\mathcal{A}$  that maximizes the expected return:  $\\mathbb{E}_{\\mathbf{s}_0\\sim \\rho ,\\mathcal{T},\\pi^*}[\\sum_{t = 0}^T\\gamma^t R_t]$ . Note that  $\\mathcal{M}_k$  and  $\\mathcal{M}$  share the same optimal value function,  $V^{*}(\\mathbf{s}) = \\max_{\\pi \\in \\Pi}\\mathbb{E}_{\\mathcal{T},\\pi}[\\sum_{k = 0}^{\\infty}\\gamma^{k}R_{t + k + 1}|\\mathbf{s}_{t} = \\mathbf{s}]$ , if they are provided with the same policy class  $\\Pi$ . A well-trained KGRL agent can possess the following properties: knowledge-acquirable, sample-efficient, generalizable, compositional, and incremental. Here we formally define these properties.\nDefinition 3.1 (Knowledge-Acquirable). An agent can acquire knowledge internally instead of only following  $\\mathcal{G}$ . We refer to this internal knowledge as an inner policy and denote it as  $\\pi_{in}(\\cdot |\\cdot):S\\to \\mathcal{A}$ Definition 3.2 (Sample-Efficient). An agent requires fewer samples to solve for  $\\mathcal{M}_k$  than for  $\\mathcal{M}$ Definition 3.3 (Generalizable). A learned policy  $\\pi (\\cdot |\\cdot ;\\mathcal{G})$  can solve similar but different tasks. Definition 3.4 (Compositional). Assume that other agents have solved for  $m$  KGMDPs,  $\\mathcal{M}_k^1, \\ldots, \\mathcal{M}_k^m$ , with external knowledge sets,  $\\mathcal{G}^1, \\ldots, \\mathcal{G}^m$ , and inner policies,  $\\pi_{in}^1, \\ldots, \\pi_{in}^m$ . An agent is compositional if it can learn to solve a KGMDP  $\\mathcal{M}_k^*$  with the external knowledge set  $\\mathcal{G}^* \\subseteq \\bigcup_{i=1}^{m} \\mathcal{G}^i \\cup \\{\\pi_{in}^1, \\ldots, \\pi_{in}^m\\}$ .\nDefinition 3.5 (Incremental). An agent is incremental if it has the following two abilities: (1) Given a KGMDP  $\\mathcal{M}_k$  for the agent to solve within  $T$  timesteps. The agent can learn to solve  $\\mathcal{M}_k$  with the external knowledge sets,  $\\mathcal{G}_1,\\ldots ,\\mathcal{G}_T$ , where  $\\mathcal{G}_t,t\\in \\{1,\\dots,T\\}$ , is the knowledge set at time step  $t$ , and  $\\mathcal{G}_t$  can be different from one another. (2) Given a sequence of KGMDPs  $\\mathcal{M}_k^1,\\ldots ,\\mathcal{M}_k^m$ , the agent can solve them with external knowledge sets,  $\\mathcal{G}^1,\\ldots ,\\mathcal{G}^m$ , where  $\\mathcal{G}^i,i\\in \\{1,\\dots,m\\}$ , is the knowledge set for task  $i$ , and  $\\mathcal{G}^i$  can be different from one another."
    },
    {
      "unique_id": "a7dbecc8-5150-45ad-83e7-8f25fb7c37a4",
      "score": 0.3542600149939414,
      "pagerank": 0.3542600149939414,
      "section_title": "1 Introduction",
      "context": "What learning capabilities do humans possess, yet RL agents still missing? Studies in social learning [4] have demonstrated that humans often observe the behavior of others in diverse situations and utilize those strategies as external knowledge to accelerate their own exploration of solution-space. This type of learning is very flexible for humans since they can freely reuse and update the knowledge they already possess. The followings are the five properties (the last four have been mentioned in [14]) that summarize the efficiency and flexibility of human learning. [Knowledge-Acquirable]: Humans can develop their strategies by observing others. [Sample-Efficient]: Humans require fewer interactions with the environment to solve a task by learning from external knowledge. [Generalizable]: Humans can apply previously observed strategies, whether developed internally or provided externally, to unseen tasks. [Compositional]: Humans can combine strategies from multiple sources to form their knowledge set. [Incremental]: Humans do not need to relearn how to navigate the entire knowledge set from scratch when they remove outdated strategies or add new ones.\nPossessing all five learning properties remains challenging for RL agents. Previous work has endowed an RL agent with the ability to learn from external knowledge (knowledge-acquirable) and mitigate sample inefficiency [21, 25, 27, 36], where the knowledge focused in this paper is state-action mappings (full definition in Section 3), including pre-collected demonstrations or policies. Among those methods, some have also allowed agents to combine policies in different forms to predict optimal actions (compositional) [25, 27]. However, these approaches may not be suitable for incremental learning, in which an agent learns a sequence of tasks using one expandable knowledge set. In such a case, whenever the knowledge set is updated by adding or replacing policies, prior methods, e.g., [27, 36], require relearning the entire multi-policy fusion process, even if the current task is similar to the previous one. This is because their designs of knowledge representations are intertwined with the knowledge-fusing mechanism, which restricts changing the number of policies in the knowledge set.\nTo this end, our goal is to enhance RL grounded on external knowledge policies with more flexibility. We first introduce Knowledge-Grounded Reinforcement Learning (KGRL), an RL paradigm that seeks to find an optimal policy of a Markov Decision Process (MDP) given a set of external policies as illustrated in Figure 1. We then formally define the knowledge-acquirable, sample-efficient, generalizable, compositional, and incremental properties that a well-trained KGRL agent can possess."
    }
  ]
}